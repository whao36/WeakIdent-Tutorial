{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-txqJ4ZK_J5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "WeakIdent Tutorial part II: PDE identification using weak form\n",
        "\n",
        "Author: Wenbo Hao\n",
        "\n",
        "Note: the whole tutorial is adapted from the WeakIdent-Python code developed by\n",
        "Mengyi Tang, available at the link below.\n",
        "https://github.com/sunghakang/WeakIdent/tree/main/WeakIdent-Python\n",
        "\n",
        "Date: 11/19/2024\n",
        "\n",
        "Introduction:\n",
        "\n",
        "This file is the second among two files of a tutorial for WeakIdent: a method\n",
        "to identify partial differential equations from spatiotemperal data leveraging\n",
        "the weak form of PDEs. This file shows the procedure to solve the identification\n",
        "problem Wc = b for coefficient vector c once we get the weak feature matrices\n",
        "W and b, including performing colomn normalization on W, selecting the support\n",
        "of c based on sparsity level and trimming error, and recovering the coefficients.\n",
        "The document didn't perform error normalization or find highly dynamic regions\n",
        "for identification.  For a more comprehensive code, see\n",
        "https://github.com/sunghakang/WeakIdent/tree/main/WeakIdent-Python.\n",
        "The paper that develops this method is on\n",
        "https://doi.org/10.1016/j.jcp.2023.112069. If you found WeakIdent useful in your\n",
        "research, please consider citing it:\n",
        "\n",
        "@article{tang2023weakident,\n",
        "  title={WeakIdent: Weak formulation for Identifying Differential Equation using\n",
        "  Narrow-fit and Trimming},\n",
        "  author={Tang, Mengyi and Liao, Wenjing and Kuske, Rachel and Kang, Sung Ha},\n",
        "  journal={Journal of Computational Physics},\n",
        "  pages={112069},\n",
        "  year={2023},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import scipy.linalg\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our feature matrix [b|W] for the Weak Form Identification problem b = Wc.\n",
        "\n",
        "# Note that the variable W defined below is the augemented matrix [b|W] but not\n",
        "# W alone. The first colomn of our variable W refers to the LHS feature b and\n",
        "# the other colomns refer to the RHS feature W.\n",
        "\n",
        "# The weakfeature matrix W is generated in the Tutorial Part I: data preparation\n",
        "# for WeakIdent\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/WeakIdent_tutorial_W.pkl', 'rb') as f:\n",
        "    W = pickle.load(f)\n",
        "\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJNysfEzMull",
        "outputId": "fdc13724-dde5-4949-cfd7-26a10055e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[[-0.49105281  0.08151466  0.04872886 ...  0.01989212  0.35632694\n",
            "   2.73898463]\n",
            " [-0.49542526  0.08151466  0.05659904 ...  0.0294907   0.40414009\n",
            "   0.96449738]\n",
            " [-0.47585739  0.08151466  0.06319881 ...  0.03966488  0.40021846\n",
            "  -1.32547417]\n",
            " ...\n",
            " [ 0.30372365  0.08151466 -0.02656423 ... -0.00302245 -0.03494819\n",
            "   0.24789304]\n",
            " [ 0.2916584   0.08151466 -0.02899285 ... -0.003777   -0.02377685\n",
            "   0.63852418]\n",
            " [ 0.27253655  0.08151466 -0.03004175 ... -0.0041389  -0.00405409\n",
            "   0.90497655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main function to identify PDE for all equations (variables)\n",
        "def diff_eqn_identification(\n",
        "        tau: float, num_of_variables: int, dim_x: int, is_1d_ode: bool,\n",
        "        W_b_large: np.array,S_b_large: np.array) -> Tuple[np.array, np.array,\n",
        "                                                          np.array]:\n",
        "\n",
        "    \"\"\"\n",
        "    This function performs identification using given feature matrix\n",
        "    (including lhs and rhs), scale matrix and row index of highly dynamic region.\n",
        "\n",
        "    Args:\n",
        "        tau (float): trimming score. Defaults to 0.05.\n",
        "        num_of_variables(int) : number of variables (equations) of given data.\n",
        "        dim_x(int): spatial dimension of given data.\n",
        "        is_1d_ode (bool): whether given data is 1d-ode type.\n",
        "        W_b_large (np.array): feature matrix.\n",
        "        S_b_large (np.array): scale matrix, used for error normalization\n",
        "\n",
        "    Returns:\n",
        "        W(np.array):  original RHS feature matrix.\n",
        "        b(np.array):  original LHS feature matrix, it can be u_t or v_t in pdes.\n",
        "        c(np.array):  identified sparse coefficient vector of size (L, n)\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the scale matrix for W_large to vectors of float 64 data type\n",
        "    # and computes the corresponding scale matrix for the lhs and rhs\n",
        "    scales_W_b = np.mean(np.abs(S_b_large), axis=0)\n",
        "    scales_lhs_features, scales_rhs_features = scales_W_b, 1/scales_W_b.flatten()\n",
        "\n",
        "    # Seperate the feature matrix W_b_large to LHS feature b and RHS feature W\n",
        "    b = W_b_large[:, :num_of_variables]\n",
        "    W = W_b_large[:, num_of_variables:]\n",
        "\n",
        "    # Performing error normalization to W and b\n",
        "    # This part is not yet implemented!!!\n",
        "    b_tilda = b\n",
        "    W_tilda = W\n",
        "\n",
        "    # Initialize the coefficient array c\n",
        "    c_pred = np.zeros((W_b_large.shape[1] - num_of_variables, num_of_variables))\n",
        "\n",
        "    # set up maximum sparsity level. For a predetermined dictionary, one can use\n",
        "    # a number <= # total features as maximum sparsity level. This number is\n",
        "    # suggested to be an appropriate number to reduce computation cost.\n",
        "    sparsity = set_sparsity_level(is_1d_ode, num_of_variables, dim_x)\n",
        "\n",
        "    for num_var in range(num_of_variables):\n",
        "        support_pred, coeff_sp = weak_ident_feature_selection(\n",
        "            W, b, b_tilda, W_tilda, sparsity, scales_rhs_features,\n",
        "            scales_lhs_features, num_var, tau)\n",
        "        c_pred[support_pred, num_var] = coeff_sp\n",
        "    return W, b, c_pred/100\n",
        "\n",
        "\n",
        "# This is the function that identifies the coefficient vector for a single\n",
        "# equation (variable).\n",
        "def weak_ident_feature_selection(W: np.array,\n",
        "                                 b: np.array,\n",
        "                                 b_tilda: np.array,\n",
        "                                 W_tilda: np.array,\n",
        "                                 k: int,\n",
        "                                 s_rhs: np.array,\n",
        "                                 s_lhs: np.float64,\n",
        "                                 num_var: int,\n",
        "                                 tau=0.05) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This function returns the support identified by WeakIdent and coefficients\n",
        "    values on this support\n",
        "\n",
        "    Args:\n",
        "        w (np.array): (feature matrix array) of shape (N, L) where L is the\n",
        "        number of rhs features and N is number of data points.\n",
        "        b (np.array): (dynamic variable u_t) of shape (N, n), where n is the num\n",
        "        of equations (variables)\n",
        "        b_tilda (np.array): error normalized vector b of shape (N, n).\n",
        "        w_tilda (np.array): error normalized matrix w of shape (N, L).\n",
        "        k (np.array): maximum allowed sparsity level.\n",
        "        s_rhs (np.array): scale vector s for rhs features, array of shape (L, ).\n",
        "        s_lhs (np.float64): scale number for lhs feature (u_t).\n",
        "        num_var: the index of equation (variable) for which we are performing\n",
        "        identification\n",
        "        tau (float, optional): trimming score. Defaults to 0.05.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: a support vector that stores the index of finalized candiate\n",
        "        features for each variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extracting the feature matrices for the one equation (variable) we are\n",
        "    # working with\n",
        "    b_one_var = b[:, num_var].reshape(-1, 1)\n",
        "    b_tilda_one_var = b_tilda[:, num_var].reshape(-1, 1)\n",
        "    s_lhs_one_var = s_lhs[num_var]\n",
        "\n",
        "    # Initialize support dictionary and list of cross validation error\n",
        "    support_list = {}\n",
        "    cv_err_list = []\n",
        "\n",
        "    # performing colomn normalization on W\n",
        "    W_column_norm = np.linalg.norm(W, axis=0).reshape(1, -1)\n",
        "    column_normalized_W = W / W_column_norm\n",
        "\n",
        "    # Iterate over maximum sparsity level. For each sparsity level returns\n",
        "    # append a support to the dictionary\n",
        "    for i in range(k):\n",
        "        support = subspace_persuit(column_normalized_W,\n",
        "                                   b_one_var / np.linalg.norm(b_one_var, 2), i)\n",
        "        c_pred = narrow_fit(W_tilda, b_tilda_one_var, support, s_rhs,\n",
        "                            s_lhs_one_var)\n",
        "        trim_score = compute_trim_score(W_column_norm, support, c_pred)\n",
        "        support_list[i] = support\n",
        "        cv_err_list.append(\n",
        "            compute_cross_validation_err_v2(support, W_tilda, b_tilda_one_var))\n",
        "\n",
        "        # performing trimming on the support, cutting the terms with trim score\n",
        "        # smaller than tau\n",
        "        while len(support) > 1:\n",
        "            idx_least_imp_feature = np.where(trim_score == trim_score.min())[0]\n",
        "            if trim_score[idx_least_imp_feature[0]] > tau:\n",
        "                break\n",
        "            idx_least_imp_feature = idx_least_imp_feature[0]\n",
        "            support = np.delete(support, idx_least_imp_feature)\n",
        "            c_pred = narrow_fit(W_tilda, b_tilda_one_var, support,\n",
        "                                s_rhs, s_lhs_one_var)\n",
        "            trim_score = compute_trim_score(W_column_norm, support, c_pred)\n",
        "            cv_err_list[i] = compute_cross_validation_err_v2(\n",
        "                support, W_tilda, b_tilda_one_var)\n",
        "            support_list[i] = support\n",
        "    # Convert the list of cross validation error to np.array and choose the\n",
        "    # support corresponding to sparsity levels with the smallest cross\n",
        "    # validation error\n",
        "    cv_err_list = np.array(cv_err_list)\n",
        "    cross_idx = np.argmin(cv_err_list)\n",
        "    support_pred = support_list[int(cross_idx)]\n",
        "\n",
        "    # Performing least square fit on the predicted support and scale the result\n",
        "    # back\n",
        "    relative_scale = s_rhs / s_lhs_one_var\n",
        "    coeff_sp = least_square_adp(\n",
        "        W_tilda[:, support_pred],\n",
        "        b_tilda_one_var) * (relative_scale[support_pred].reshape(-1, 1))\n",
        "\n",
        "    return support_pred, coeff_sp.flatten()\n",
        "\n",
        "\n",
        "# The function performs least-square fitting\n",
        "def least_square_adp(A: np.array, b: np.array) -> np.array:\n",
        "\n",
        "    \"\"\"\n",
        "    This script solves least square problem Ax = b. In the case of A and b both\n",
        "    being a constant number,\n",
        "    it simply performs division x = b / A.\n",
        "\n",
        "    Args:\n",
        "        A (np.array): array of shape (N, L).\n",
        "        b (np.array): array of shape (N, n).\n",
        "\n",
        "    Returns:\n",
        "        np.array: array of shape (L, n).\n",
        "    \"\"\"\n",
        "\n",
        "    if A.shape[0] == 1:\n",
        "        x = b / A\n",
        "    else:\n",
        "        x, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
        "    return x\n",
        "\n",
        "\n",
        "# This function is used to set up an appropriate maximum sparsity level for a\n",
        "# differential equation.\n",
        "def set_sparsity_level(is_1d_ode: bool, num_of_u: int, dim_x: int) -> int:\n",
        "  \"\"\"This function set maximum sparsity level for support recovery.\n",
        "\n",
        "  Args:\n",
        "      is_1d_ode (bool): whether or not given data is 1d ode data.\n",
        "      num_of_u (int): number of variables.\n",
        "      dim_x (int): spatial dimension.\n",
        "\n",
        "  Returns:\n",
        "      int: sparsity level\n",
        "  \"\"\"\n",
        "  if is_1d_ode:\n",
        "      if num_of_u <= 2:\n",
        "          sparsity = 10\n",
        "      else:\n",
        "          sparsity = 15\n",
        "  else:\n",
        "      if num_of_u == 2 and dim_x == 2:\n",
        "          sparsity = 25\n",
        "      else:\n",
        "          sparsity = 10\n",
        "  return sparsity\n",
        "\n",
        "# This function computes the trim score used for trimming\n",
        "def compute_trim_score(w_column_norm: np.array, support: np.array,\n",
        "                       c_pred: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    This function computes the trimming score vector for given support and\n",
        "    coefficient values.\n",
        "    Args:\n",
        "        w_column_norm (np.array): column norm vector of feature matrix w\n",
        "        support (np.array): a support vector that stores the index of candiate\n",
        "        features for each variable.\n",
        "        c_pred (np.array): a coefficient vector that stores the values of\n",
        "        candiate features for each variable.\n",
        "\n",
        "    Returns:\n",
        "        np.array: a trimming score vector for each candidate feature\n",
        "    \"\"\"\n",
        "    trim_score = w_column_norm.flatten()[support] * c_pred.flatten()\n",
        "    trim_score = trim_score / np.max(trim_score)\n",
        "    return trim_score\n",
        "\n",
        "\n",
        "# This funciton performs least square fitting once a support is identified\n",
        "def narrow_fit(w: np.array, b: np.array, support: np.array, s_rhs: np.array,\n",
        "               s_lhs: np.float64) -> np.array:\n",
        "    \"\"\"\n",
        "    This function perform narrow least square fit for the problem wb = c.\n",
        "    Notice that here w and c have to be error normalized features. Rescaling\n",
        "    step should be done before this\n",
        "    function is called. Narrow-fit is proposed in WeakIdent to help increase\n",
        "    the accuracy of coefficients. A\n",
        "    hyghly dynamic region is found and scaling matrix is calculated to compute\n",
        "    error normalized feature matrix and normalized dynamic variable.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        w (np.array): \\Tilda{W}_narrow, error normalized feature matrix, array\n",
        "        of shape (NxNt, L).\n",
        "        b (np.array): \\Tilda{b}_narrow, error normalized dynamic variable, array\n",
        "        of shape (NxNt, 1).\n",
        "        support (np.array): a support vector that stores the index of candiate\n",
        "        features for each variable.\n",
        "        s_rhs (np.array): scale vector s for rhs features, array of shape (L, ).\n",
        "        s_lhs (np.float64): scale number for lhs feature (u_t).\n",
        "\n",
        "    Returns:\n",
        "        np.array: the coefficient values for a given support\n",
        "    \"\"\"\n",
        "    c_pred = least_square_adp(w[:, support], b)\n",
        "    c_pred = np.absolute(c_pred * s_rhs[support].reshape(-1, 1) / s_lhs)\n",
        "    return c_pred\n",
        "\n",
        "\n",
        "# This function finds a support given a sparsity level\n",
        "def subspace_persuit(phi: np.array, b: np.array, k: int) -> np.array:\n",
        "\n",
        "  \"\"\"\n",
        "  This function finds the support for a sparse vector c such that phi * x = b\n",
        "  where k numbers in c\n",
        "  are nonzero. Note: Subspace Persuit is a greedy algorithm which minimizes\n",
        "  residual error when searching\n",
        "  for a support.\n",
        "\n",
        "  Args:\n",
        "      phi (np.array): the RHS feature matrix W, array of shape (N, L)\n",
        "      b (np.array): LHS feature matrix b, array of shape (N, 1)\n",
        "      k (int): sparse level\n",
        "\n",
        "  Returns:\n",
        "      np.array: array of shape (k,). a support vector that stores the index of\n",
        "      candiate features for each variable.\n",
        "  \"\"\"\n",
        "\n",
        "  itermax = 15\n",
        "  n = len(phi[0])\n",
        "  is_disp = 0\n",
        "  b_t = np.transpose(b)\n",
        "  cv = np.absolute(np.matmul(b_t, phi))\n",
        "  cv_index = np.argsort(-cv).flatten()\n",
        "  lda = cv_index[:(k + 1)]\n",
        "  phi_lda = phi[:, lda]\n",
        "  if is_disp:\n",
        "      print(np.sort(lda))\n",
        "  x = least_square_adp(np.matmul(np.transpose(phi_lda), phi_lda),\n",
        "                        np.matmul(np.transpose(phi_lda), b).reshape(-1, 1))\n",
        "  r = b - np.matmul(phi_lda, x)\n",
        "  res = np.linalg.norm(r, 2)\n",
        "  if res < 10**(-12):\n",
        "      X = np.zeros(n)\n",
        "      X[lda] = np.transpose(x)\n",
        "      supp = [i for i in range(phi.shape[1]) if X[i] != 0]\n",
        "      return np.array(supp)\n",
        "  usedlda = np.zeros(n)\n",
        "  usedlda[lda] = 1\n",
        "  for _ in range(itermax):\n",
        "      res_old = res\n",
        "      cv = np.absolute(np.matmul(np.transpose(r), phi))\n",
        "      cv_index = np.argsort(-cv).flatten()\n",
        "      sga = np.union1d(lda, cv_index[:k + 1])\n",
        "      phi_sga = phi[:, sga]\n",
        "      x_temp = least_square_adp(np.matmul(np.transpose(phi_sga), phi_sga),\n",
        "                                np.matmul(np.transpose(phi_sga), b))\n",
        "      x_temp_index = np.argsort(-np.absolute(x_temp.flatten())).flatten()\n",
        "      lda = sga[x_temp_index[:k + 1]]\n",
        "      phi_lda = phi[:, lda]\n",
        "      usedlda[lda] = 1\n",
        "      x = least_square_adp(np.matmul(np.transpose(phi_lda), phi_lda),\n",
        "                            np.matmul(np.transpose(phi_lda), b))\n",
        "      r = b - np.matmul(phi_lda, x)\n",
        "      res = np.linalg.norm(r, 2)\n",
        "      X = np.zeros(n)\n",
        "      X[lda] = np.transpose(x)\n",
        "      if res / res_old >= 1 or res < 10**(-12):\n",
        "          supp = [i for i in range(phi.shape[1]) if X[i] != 0]\n",
        "          return np.array(supp)\n",
        "\n",
        "\n",
        "# This function computes the cross validation error (from a random split w.r.t.\n",
        "# ratio 1/100).\n",
        "def compute_cross_validation_err_v2(support: np.array,\n",
        "                                    w: np.array,\n",
        "                                    b: np.array,\n",
        "                                    iter_max=30) -> np.float64:\n",
        "\n",
        "    \"\"\"\n",
        "    This function returns the cross validation error for the support of vector c\n",
        "    in the least square problem Wc = b.\n",
        "    Note: Cross-validation error is computed over 30 times and take mean + std\n",
        "    as our final result for a stablized error.\n",
        "    Args:\n",
        "        support (np.array): a support vector that stores the index of candiate\n",
        "        features for each variable.\n",
        "        w (np.array): error normalized matrix w of shape (N, L).\n",
        "        b (np.array): error normalized vector b of shape (N, 1).\n",
        "        iter_max (int, optional): maximum number of iterations. Defaults to 30.\n",
        "\n",
        "    Returns:\n",
        "        np.float64: cross-validation error (modified version).\n",
        "    \"\"\"\n",
        "\n",
        "    err_cross_accu = []\n",
        "    for _ in range(iter_max):\n",
        "        err_cross_accu.append(compute_cross_validation_error(support, w, b))\n",
        "    err_cross_accu = np.array(err_cross_accu)\n",
        "    err = np.mean(err_cross_accu) + np.std(err_cross_accu)\n",
        "    return err\n",
        "\n",
        "\n",
        "def compute_cross_validation_error(support: np.array,\n",
        "                                  w: np.array,\n",
        "                                  b: np.array,\n",
        "                                  ratio=1 / 100) -> np.float64:\n",
        "\n",
        "  \"\"\"\n",
        "  This function computes the cross validation error (from a random split w.r.t.\n",
        "  ratio 1/100).\n",
        "\n",
        "  Args:\n",
        "      support (np.array): a support vector that stores the index of candiate\n",
        "      features for each variable.\n",
        "      w (np.array): error normalized matrix w of shape (N, L).\n",
        "      b (np.array): error normalized vector b of shape (N, 1).\n",
        "      ratio (_type_, optional): ratio between two partitions of w. Defaults to\n",
        "      1/100.\n",
        "\n",
        "  Returns:\n",
        "      np.float64: cross-validation error\n",
        "  \"\"\"\n",
        "\n",
        "  n = len(b)\n",
        "  inds = np.random.permutation(n)\n",
        "  k = int(np.floor(n * ratio - 1))  # end of part 1\n",
        "  w = w[inds, :]\n",
        "  b = b[inds, :]\n",
        "  coeff = np.zeros(w.shape[1])\n",
        "  coeff[support] = least_square_adp(w[:k, support], b[:k]).flatten()\n",
        "  e1 = np.linalg.norm(w[k:, :] @ coeff.reshape(-1, 1) - b[k:])\n",
        "  coeff = np.zeros(w.shape[1])\n",
        "  coeff[support] = least_square_adp(w[k:, support], b[k:]).flatten()\n",
        "  e2 = np.linalg.norm(w[:k, :] @ coeff.reshape(-1, 1) - b[:k])\n",
        "  return e1 * (1 - ratio) + e2 * ratio"
      ],
      "metadata": {
        "id": "ij2Z11HYSPsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now we test the PDE identificaion function using our feature matrix W generated\n",
        "from the following convection diffusion equation:\n",
        "u_{t} =  - a*u_x + niu*u_{xx} with a = 1, niu = 0.1\n",
        "'''\n",
        "\n",
        "# Initialize parameters for identification\n",
        "tau = 0.05 # default the trimming threshold to 0.05\n",
        "\n",
        "# Parameters related to the PDE\n",
        "num_of_variables = 1 # num of equations (variables) to identify\n",
        "dim_x = 1 # sptial dimension of given data\n",
        "is_1d_ode = False\n",
        "W_b_large = W\n",
        "S_b_large = np.eye(W.shape[1] - num_of_variables) # Now we didn't implement the\n",
        "# error normalization part and regard the error normalization matrix as the\n",
        "# identity matrix\n",
        "\n",
        "# Performing PDE identification using the model\n",
        "W, b, c = diff_eqn_identification(tau, num_of_variables, dim_x, is_1d_ode,\n",
        "                                  W_b_large, S_b_large)\n",
        "\n",
        "\n",
        "# Printing our the solution\n",
        "#print(\"RHS feature W: \\n\", W)\n",
        "#print(\"\\n LHS feature b: \\n\", b)\n",
        "print(\"\\n Predicted coefficient vector: \\n\", c)\n",
        "print(\"\\n Actual coefficient vector: \\n\",\n",
        "      np.array([0, 0, -1, 0.1, 0, 0, 0, 0, 0, 0]).reshape(-1, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5P3eQzF08OB",
        "outputId": "dd23bb23-a8d4-426e-84b9-bf02a717a805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Predicted coefficient vector: \n",
            " [[ 0.        ]\n",
            " [ 0.        ]\n",
            " [-0.99982319]\n",
            " [ 0.09998591]\n",
            " [ 0.        ]\n",
            " [ 0.        ]\n",
            " [ 0.        ]\n",
            " [ 0.        ]\n",
            " [ 0.        ]\n",
            " [ 0.        ]]\n",
            "\n",
            " Actual coefficient vector: \n",
            " [[ 0. ]\n",
            " [ 0. ]\n",
            " [-1. ]\n",
            " [ 0.1]\n",
            " [ 0. ]\n",
            " [ 0. ]\n",
            " [ 0. ]\n",
            " [ 0. ]\n",
            " [ 0. ]\n",
            " [ 0. ]]\n"
          ]
        }
      ]
    }
  ]
}